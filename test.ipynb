{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmtrack import get_llm\n",
    "\n",
    "llm = get_llm(\"openai/gpt-4o-mii\")\n",
    "print(llm.generate(\"generate a positve word\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test groq API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    "    temperature= 0.7,\n",
    "    max_tokens=20,\n",
    "    top_p=0.9,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None,\n",
    "    n=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models, also known as efficient language models or accelerated language models, have gained significant attention in\n",
      "{'completion_tokens': 20, 'prompt_tokens': 18, 'total_tokens': 38, 'completion_time': 0.016666667, 'prompt_time': 0.006427718, 'queue_time': 0.20464392, 'total_time': 0.023094385}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(completion.choices[0].message.content)\n",
    "except:\n",
    "    print(\"Error\")\n",
    "    \n",
    "try:\n",
    "    usage = completion.usage.to_dict()\n",
    "    print(usage)\n",
    "    assert \"prompt_tokens\" in usage\n",
    "    assert \"completion_tokens\" in usage\n",
    "    assert \"total_tokens\" in usage\n",
    "except:\n",
    "    print(\"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Huggingface Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinzheli/miniconda3/envs/langchain116/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import time\n",
    "from threading import Thread\n",
    "\n",
    "device ='mps'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", padding_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", device_map=device)\n",
    "model=model.eval()\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "model.generation_config.pad_token_id=tokenizer.eos_token_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,   2705,   5020,    220,   2366,     19,    271,   2675,    527,\n",
      "            264,  11190,  18328, 128009, 128006,    882, 128007,    271,  14965,\n",
      "            527,    499,     30, 128009, 128006,  78191, 128007,    271],\n",
      "        [128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,   2705,   5020,    220,   2366,     19,    271,   2675,    527,\n",
      "            264,  11190,  18328, 128009, 128006,    882, 128007,    271,  14965,\n",
      "            527,    499,     30, 128009, 128006,  78191, 128007,    271]],\n",
      "       device='mps:0')\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 06 Oct 2024\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "who are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 06 Oct 2024\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "who are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenization time: 0.03133797645568848\n"
     ]
    }
   ],
   "source": [
    "verb_time = True\n",
    "user_input= \"who are you?\"\n",
    "chats=[[{\"role\": \"system\", \"content\": \"You are a helpful assistant\"}, \n",
    "      {\"role\": \"user\", \"content\": user_input}]]\n",
    "\n",
    "# tokenization\n",
    "if verb_time:\n",
    "    t0_1=time.time()\n",
    "encoded_input=tokenizer.apply_chat_template(chats*2, return_tensors=\"pt\" ,\n",
    "                                        add_generation_prompt=True,\n",
    "                                        padding=True,\n",
    "                                        return_dict=True).to(device)\n",
    "# print(encoded_input['input_ids'])\n",
    "print(tokenizer.batch_decode(encoded_input['input_ids'])[0])\n",
    "# print(tokenizer.batch_decode(encoded_input['input_ids'])[1])\n",
    "\n",
    "\n",
    "if verb_time:\n",
    "    t0_2=time.time()\n",
    "    print(\"Tokenization time:\",t0_2 - t0_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinzheli/miniconda3/envs/langchain116/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/Users/xinzheli/miniconda3/envs/langchain116/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/Users/xinzheli/miniconda3/envs/langchain116/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Both `max_new_tokens` (=50) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 1.9465217590332031\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "generation_config=dict(           \n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        max_new_tokens =50,\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,  \n",
    "        do_sample=False,\n",
    "        temperature=0.9,\n",
    "        top_p=0.7,\n",
    "        top_k=40,\n",
    "        num_beams=1,)\n",
    "if verb_time:\n",
    "    t1=time.time()\n",
    "\n",
    "generation_output = model.generate(\n",
    "    encoded_input[\"input_ids\"], \n",
    "    attention_mask=encoded_input[\"attention_mask\"],\n",
    "    output_scores= False,\n",
    "    return_dict_in_generate=True,\n",
    "    **generation_config,\n",
    ")\n",
    "\n",
    "if verb_time:\n",
    "    t2=time.time()\n",
    "    print (\"Inference time:\", t2-t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['sequences', 'past_key_values'])\n",
      "All: \n",
      "['system\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 06 Oct 2024\\n\\nYou are a helpful assistantuser\\n\\nwho are you?assistant\\n\\nI\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"', 'system\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 06 Oct 2024\\n\\nYou are a helpful assistantuser\\n\\nwho are you?assistant\\n\\nI\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"']\n",
      "Only LLM Generation: \n",
      "['I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"', 'I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"']\n"
     ]
    }
   ],
   "source": [
    "print(generation_output.keys())\n",
    "# print(generation_output.scores[0].shape)\n",
    "print('All: ')\n",
    "print(tokenizer.batch_decode(generation_output.sequences, skip_special_tokens=True) )\n",
    "\n",
    "print(\"Only LLM Generation: \")\n",
    "decoded_texts = [ tokenizer.decode(generation_output.sequences[i][len(encoded_input[\"input_ids\"][i]):], skip_special_tokens=True) for i in range(len(generation_output.sequences))]\n",
    "print(decoded_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = [0,1,2,]\n",
    "lst[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization time: 0.0013570785522460938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 1.641625165939331\n",
      "Tokenization time: 0.0009062290191650391\n",
      "Inference time: 19.768184900283813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GenerateOutput(text=['I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"', \"I'm an artificial intelligence assistant, and I'm here to provide information, answer questions, and help with tasks to the best of my abilities. I'm a large language model, which means I was trained on a massive dataset of text from the internet, books, and other sources.\\n\\nI don't have a personal identity or emotions like humans do, but I'm designed to be helpful and assist with a wide range of topics and requests. My main goal is to provide accurate and reliable information, and to help users like you get the information they need.\\n\\nSome of the things I can do include:\\n\\n* Answering questions on a wide range of topics, from science and history to entertainment and culture\\n* Providing definitions and explanations for words and phrases\\n* Offering grammar and spelling checks to help with writing and communication\\n* Generating text on a given topic or prompt\\n* Summarizing long pieces of text into shorter, more digestible versions\\n* Offering suggestions and ideas for projects and tasks\\n\\nFeel free to ask me anything, and I'll do my best to help!\"], log_prob=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = HFModel(model_name= \"meta-llama/Llama-3.2-1B-Instruct\", )\n",
    "\n",
    "model._ssgenerate('who are you?', verb_time=True, num_return_sequences=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain116",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
